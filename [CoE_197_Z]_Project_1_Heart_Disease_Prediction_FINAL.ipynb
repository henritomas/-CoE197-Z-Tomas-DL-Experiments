{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[CoE 197-Z] Project 1 - Heart-Disease Prediction - FINAL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "HWq-r0Hs97bb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#[COE 197-Z] Project 1: Heart Disease Prediction Model\n",
        "\n",
        "---\n",
        "\n",
        "##Competition Score : *0.29193 Log Loss*\n",
        "##Leaderboard Rank : *10th / 1444*\n",
        "---\n",
        "\n",
        "The following model was built both as a submission to the DrivenData [Machine Learning with Heart Competition](https://www.drivendata.org/competitions/54/machine-learning-with-a-heart/) \n",
        "\n",
        "and as a project under  CoE 197-Z Deep Learning 2S 1819AY. \n",
        "\n",
        "Dataset used was provided by DrivenData, and is publically available at the UCI Machine Learning Repository linked [here](https://archive.ics.uci.edu/ml/datasets/heart+Disease).\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Table of Contents**\n",
        "\n",
        "*  Preprocessing Data using Pandas\n",
        "*  Implementing K-Fold Validation \n",
        "*  Building the Deep Learning Model (3-Layer MLP)\n",
        "*  Preparing Callbacks (Model Checkpoint, LR Scheduler)\n",
        "*  Training the Model\n",
        "*   Evaluating the Model and Submission\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "goqAZnNEw-dr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Preprocess Data using Pandas**\n",
        "\n",
        "Note: Since I have zero background on preprocessing tabular data, the following was used as the main reference for this section: \n",
        "[Preprocessing Tabular Data](https://github.com/AnneDeGraaf/DrivenData_WarmUp_HeartDisease/blob/master/data_processing.py?fbclid=IwAR3Spxx1yyaRRpyO2yPeajdlv3SgcWuy-9ZwPLW5SPTWNIpzr0TFtph5h38)\n",
        "Credits to  .[AnneDeGraaf](https://github.com/AnneDeGraaf/)\n",
        "\n",
        "Pre-processing Summary:\n",
        "*  Use pandas dataframes from reading .csv datasets\n",
        "*  Normalize numerical values\n",
        "*  Change categorical data into one-hot vectors \n",
        "*  Categorical Embedding was considered but not implemented due to references stating it had no significant effect on this particular dataset.\n",
        "*  Save pre-processed data into new .csv to be loaded later"
      ]
    },
    {
      "metadata": {
        "id": "lAXLBd829MUz",
        "colab_type": "code",
        "outputId": "ad82e095-ef42-4ef8-a773-555de1b82817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Dense, Dropout, Input, BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
        "from keras.constraints import unit_norm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "train_url = 'https://raw.githubusercontent.com/henritomas/CoE197-Z-Tomas-DL-Experiments/master/train_values.csv'\n",
        "test_url = 'https://raw.githubusercontent.com/henritomas/CoE197-Z-Tomas-DL-Experiments/master/test_values.csv'\n",
        "rawTrain = pd.read_csv(train_url)\n",
        "rawTest = pd.read_csv(test_url)\n",
        "\n",
        "# change categorical data into one-hot:\n",
        "trainSlope_oneHot = pd.get_dummies(rawTrain['slope_of_peak_exercise_st_segment'], prefix='slope')\n",
        "trainThal_oneHot = pd.get_dummies(rawTrain['thal'])\n",
        "trainChestPain_oneHot = pd.get_dummies(rawTrain['chest_pain_type'], prefix='chestPain')\n",
        "trainResting_oneHot = pd.get_dummies(rawTrain['resting_ekg_results'], prefix='restingEkg')\n",
        "testSlope_oneHot = pd.get_dummies(rawTest['slope_of_peak_exercise_st_segment'], prefix='slope')\n",
        "testThal_oneHot = pd.get_dummies(rawTest['thal'])\n",
        "testChestPain_oneHot = pd.get_dummies(rawTest['chest_pain_type'], prefix='chestPain')\n",
        "testResting_oneHot = pd.get_dummies(rawTest['resting_ekg_results'], prefix='restingEkg')\n",
        "\n",
        "# replace categorical columns by one-hot\n",
        "rawTrain.drop(['slope_of_peak_exercise_st_segment','thal','chest_pain_type','resting_ekg_results'], axis=1, inplace=True)\n",
        "rawTrain = rawTrain.join([trainSlope_oneHot, trainThal_oneHot, trainChestPain_oneHot, trainResting_oneHot])\n",
        "rawTest.drop(['slope_of_peak_exercise_st_segment','thal','chest_pain_type','resting_ekg_results'], axis=1, inplace=True)\n",
        "rawTest = rawTest.join([testSlope_oneHot, testThal_oneHot, testChestPain_oneHot, testResting_oneHot])\n",
        "\n",
        "# check for NaN's in dataset\n",
        "print(rawTrain.isnull().values.any())\n",
        "print(rawTest.isnull().values.any())\n",
        "\n",
        "# apply normalization to numerical data\n",
        "numCols = ['resting_blood_pressure', 'serum_cholesterol_mg_per_dl', 'oldpeak_eq_st_depression', 'age', 'max_heart_rate_achieved']\n",
        "for col in numCols:\n",
        "\trawTest[col] = (rawTest[col] - rawTrain[col].mean()) / rawTrain[col].std()\n",
        "\trawTrain[col] = (rawTrain[col] - rawTrain[col].mean()) / rawTrain[col].std()\n",
        "\tprint(rawTrain[col].mean(), rawTrain[col].std()) # should be 0 and 1\n",
        "\n",
        "# Storing processed data into new file\n",
        "rawTrain.to_csv('../train_values_normalized.csv')\n",
        "rawTest.to_csv('../test_values_normalized.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "False\n",
            "4.354541418807558e-16 1.0\n",
            "4.502571155424246e-17 1.0\n",
            "6.1679056923619804e-18 0.9999999999999992\n",
            "1.0986582014519779e-16 0.9999999999999994\n",
            "5.896517841898053e-16 1.0000000000000004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yg_qi2KD29-V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Splitting Data for 8-Fold Cross Validation**\n",
        "*  use StratifiedKFold from scikit to split data into 8 folds\n",
        "*  final processing of data: convert test labels into one-hot vector, drop patient id column in training data\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vzCmGMGj29Vg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data_kfold(k):\n",
        "  #Load pre-processed/ normalized data, mark column 0 as the index (patiend id)\n",
        "  train_labels_url = 'https://raw.githubusercontent.com/henritomas/CoE197-Z-Tomas-DL-Experiments/master/train_labels.csv'\n",
        "  x_train = pd.read_csv('../train_values_normalized.csv', index_col=0)\n",
        "  y_train = pd.read_csv(train_labels_url, index_col=0)\n",
        "  \n",
        "  folds = list(StratifiedKFold(n_splits=k, shuffle=True, random_state=1).split(x_train, y_train))\n",
        "  \n",
        "  #Reshape/Format data\n",
        "  num_labels = len(np.unique(y_train))\n",
        "  y_train = to_categorical(y_train)\n",
        "  x_train = x_train.drop('patient_id',1) #Drops/deletes patient_id column\n",
        "  \n",
        "  return folds, x_train, y_train\n",
        "\n",
        "k = 8\n",
        "folds, x_train, y_train = load_data_kfold(k)\n",
        "num_labels=2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wxoVVEF5tP6k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Build the Deep Learning Model **\n",
        "\n",
        "*   Set random seeds to constant values for reproducibility.\n",
        "\n",
        "*   Pick 3-Layer MLP as the model due to its advantages over CNN and RNN in classfying tabular data as opposed to image or sequential data. \n",
        "\n",
        "*   Batch Size = 128 to achieve sharper but lower log loss. \n",
        "\n",
        "*   Applied L2 norm as a regularizer, then unit norm as a constraint.\n",
        "\n",
        "*   ReLU as activation function, Softmax for last layer\n",
        "\n",
        "*   main reference for customizing Adam optimizer [here](https://www.kaggle.com/jasontsmith2718/predicting-heart-disease?fbclid=IwAR121u3BDOoqiwx-vSuFFiNJPP6VdfpYIYu1c61eM_mEe4gD4V1WD6fUz_s)\n",
        "\n",
        "*   Introducing BatchNorm + Dropout makes model more accurate but less confident in predictions, (higher accuracy, BUT higher log loss error)\n",
        "\n",
        "*   Introducing weight initializers (he and glorot in particular) returned flatter loss at model convergence, but slightly higher loss in general, so it was not used."
      ]
    },
    {
      "metadata": {
        "id": "uga5G1FEjvRz",
        "colab_type": "code",
        "outputId": "fb5ad774-ccf6-4283-c67f-a9b247742222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "#Reproducibility Seeds\n",
        "np.random.seed(5318)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(5318)\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 20\n",
        "\n",
        "def build_model():\n",
        "  #Network Parameters\n",
        "  input_dim = (x_train.shape[1],) #required to be a tuple\n",
        "\n",
        "  kreg = l2(0.0001)\n",
        "\n",
        "  #Build Model\n",
        "  inputs = Input(shape=input_dim)\n",
        "  y = Dense(32,\n",
        "            input_dim=input_dim,\n",
        "            activation='relu',\n",
        "            kernel_regularizer=kreg,\n",
        "            kernel_constraint=unit_norm())(inputs)\n",
        "  \n",
        "  y = Dense(16,\n",
        "            input_dim=input_dim,\n",
        "            activation='relu',\n",
        "            kernel_regularizer=kreg)(y)\n",
        "  \n",
        "  outputs = Dense(num_labels, activation='softmax',\n",
        "                 kernel_regularizer=kreg,\n",
        "                 kernel_constraint=unit_norm())(y)\n",
        "  opt = Adam(lr=0.0001, \n",
        "             beta_1=0.9, \n",
        "             beta_2=0.999, \n",
        "             epsilon=1e-7, \n",
        "             decay=0.0, \n",
        "             amsgrad=False)\n",
        "  model = Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer=opt,\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         (None, 22)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 32)                736       \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 1,298\n",
            "Trainable params: 1,298\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vtMZkoWUuWJo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Prepare Callbacks **\n",
        "\n",
        "*   Uses Model Checkpoints to save the model at the epoch with the lowest validation loss. \n",
        "*   Uses Learning Rate Scheduler to tweak the learning rate at specific epochs."
      ]
    },
    {
      "metadata": {
        "id": "tpXezV6SuVk8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#define LR scheduler\n",
        "def scheduler(epoch):\n",
        "  if epoch <= 9:\n",
        "    new_lr = 0.009962\n",
        "  elif 9 < epoch <= 11:\n",
        "    new_lr = 0.0052\n",
        "  elif epoch > 11:\n",
        "    new_lr = 0.0001\n",
        "    \n",
        "  return new_lr\n",
        "  \n",
        "\n",
        "#checkpoint saves the model with the minimum val_loss\n",
        "def get_callbacks(name_weights, patience_lr):\n",
        "    mcp_save = ModelCheckpoint(name_weights, save_best_only=True, monitor='val_loss', mode='min', save_weights_only=False)\n",
        "    lrate = LearningRateScheduler(scheduler)\n",
        "    #reduce_lr_loss = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=patience_lr, verbose=1, epsilon=1e-4, mode='min')\n",
        "    return [mcp_save, lrate]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M1oKN8_1Lgnl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Train the Model (3-Layer MLP) with 8-Fold CV**\n",
        "\n",
        "*  From previous training with 8-fold CV, it was determined that **Fold 3 returns the most accurate models**, and thus **training for this part will only be done on a Fold 3 evaluation**.\n",
        "\n",
        "*  Learning Rate was tweaked to be large at the first few epochs to skip bad local minima, and then changed smaller later on to converge at good sharp minimas for better loss. \n",
        "\n",
        "*  Model Checkpoints is set to save the model at the epoch where the validation loss is lowest.\n",
        "\n",
        "*  Model is trained on 157 samples and validated on only 23 samples. \n",
        "\n",
        "*  For better generalization, it is observed that the training loss should be around ~0.32 log loss while the validation loss should be < 0.2 log loss. Anywhere past this and it seems that the model is overfitting on the training data and competition score gets worse.\n"
      ]
    },
    {
      "metadata": {
        "id": "gz14wsN_mjNc",
        "colab_type": "code",
        "outputId": "e54e4d68-f66e-4998-e6a0-805c9b0ea1b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "cell_type": "code",
      "source": [
        "kfold_summary = {} #Save results of k-fold validation for each fold here\n",
        "for j, (train_idx, val_idx) in enumerate(folds):\n",
        "    \n",
        "    #If not Fold 3, Skip.\n",
        "    if j != 3:\n",
        "      continue\n",
        "    \n",
        "    print('\\nFold ',j)\n",
        "    #since x_train is a pandas dataframe, to access its folds, \"df.iloc[]\" is required\n",
        "    x_train_cv = x_train.iloc[train_idx] \n",
        "    y_train_cv = y_train[train_idx]\n",
        "    x_valid_cv = x_train.iloc[val_idx]\n",
        "    y_valid_cv= y_train[val_idx]\n",
        "    \n",
        "    name_weights = \"final_model_fold\" + str(j) + \".h5\"\n",
        "    callbacks = get_callbacks(name_weights=name_weights, patience_lr=2)\n",
        "    model = build_model()\n",
        "\n",
        "    history = model.fit(x_train_cv, y_train_cv,\n",
        "                        validation_data=(x_valid_cv, y_valid_cv),\n",
        "                        epochs=epochs,\n",
        "                        batch_size=batch_size, \n",
        "                        callbacks=callbacks)\n",
        "    \n",
        "    kth_eval = model.evaluate(x_valid_cv, y_valid_cv)\n",
        "    print(kth_eval)\n",
        "    kfold_summary[j] = kth_eval\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold  3\n",
            "Train on 157 samples, validate on 23 samples\n",
            "Epoch 1/20\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.7456 - acc: 0.4331 - val_loss: 0.5798 - val_acc: 0.8261\n",
            "Epoch 2/20\n",
            "157/157 [==============================] - 0s 128us/step - loss: 0.6164 - acc: 0.7197 - val_loss: 0.4977 - val_acc: 0.8696\n",
            "Epoch 3/20\n",
            "157/157 [==============================] - 0s 123us/step - loss: 0.5486 - acc: 0.7962 - val_loss: 0.4372 - val_acc: 0.8696\n",
            "Epoch 4/20\n",
            "157/157 [==============================] - 0s 118us/step - loss: 0.5022 - acc: 0.8025 - val_loss: 0.3779 - val_acc: 0.8696\n",
            "Epoch 5/20\n",
            "157/157 [==============================] - 0s 99us/step - loss: 0.4605 - acc: 0.8280 - val_loss: 0.3254 - val_acc: 0.8696\n",
            "Epoch 6/20\n",
            "157/157 [==============================] - 0s 157us/step - loss: 0.4294 - acc: 0.8408 - val_loss: 0.2812 - val_acc: 0.9130\n",
            "Epoch 7/20\n",
            "157/157 [==============================] - 0s 132us/step - loss: 0.3996 - acc: 0.8535 - val_loss: 0.2417 - val_acc: 0.9565\n",
            "Epoch 8/20\n",
            "157/157 [==============================] - 0s 100us/step - loss: 0.3789 - acc: 0.8599 - val_loss: 0.2097 - val_acc: 0.9565\n",
            "Epoch 9/20\n",
            "157/157 [==============================] - 0s 105us/step - loss: 0.3655 - acc: 0.8535 - val_loss: 0.1918 - val_acc: 0.9565\n",
            "Epoch 10/20\n",
            "157/157 [==============================] - 0s 149us/step - loss: 0.3528 - acc: 0.8535 - val_loss: 0.1807 - val_acc: 0.9565\n",
            "Epoch 11/20\n",
            "157/157 [==============================] - 0s 125us/step - loss: 0.3402 - acc: 0.8726 - val_loss: 0.1767 - val_acc: 0.9565\n",
            "Epoch 12/20\n",
            "157/157 [==============================] - 0s 127us/step - loss: 0.3332 - acc: 0.8662 - val_loss: 0.1752 - val_acc: 0.9565\n",
            "Epoch 13/20\n",
            "157/157 [==============================] - 0s 184us/step - loss: 0.3288 - acc: 0.8790 - val_loss: 0.1752 - val_acc: 0.9565\n",
            "Epoch 14/20\n",
            "157/157 [==============================] - 0s 172us/step - loss: 0.3287 - acc: 0.8790 - val_loss: 0.1753 - val_acc: 0.9565\n",
            "Epoch 15/20\n",
            "157/157 [==============================] - 0s 146us/step - loss: 0.3286 - acc: 0.8790 - val_loss: 0.1753 - val_acc: 0.9565\n",
            "Epoch 16/20\n",
            "157/157 [==============================] - 0s 188us/step - loss: 0.3285 - acc: 0.8790 - val_loss: 0.1753 - val_acc: 0.9565\n",
            "Epoch 17/20\n",
            "157/157 [==============================] - 0s 137us/step - loss: 0.3284 - acc: 0.8790 - val_loss: 0.1753 - val_acc: 0.9565\n",
            "Epoch 18/20\n",
            "157/157 [==============================] - 0s 147us/step - loss: 0.3282 - acc: 0.8790 - val_loss: 0.1753 - val_acc: 0.9565\n",
            "Epoch 19/20\n",
            "157/157 [==============================] - 0s 129us/step - loss: 0.3280 - acc: 0.8790 - val_loss: 0.1754 - val_acc: 0.9565\n",
            "Epoch 20/20\n",
            "157/157 [==============================] - 0s 166us/step - loss: 0.3279 - acc: 0.8790 - val_loss: 0.1754 - val_acc: 0.9565\n",
            "23/23 [==============================] - 0s 231us/step\n",
            "[0.17539602518081665, 0.95652174949646]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7bFu8F2WQxcr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Evaluating the Model and Submission**\n",
        "\n",
        "*  Evaluate Model on an approximation of the competition's test data.\n",
        "*  Store probability predictions in a csv for submission."
      ]
    },
    {
      "metadata": {
        "id": "7nUN1oO78oQk",
        "colab_type": "code",
        "outputId": "c3922236-be8c-4f73-eac7-e8224497be71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "#Loads Final Test Data used on competition evaluation\n",
        "raw_z_test = pd.read_csv('../test_values_normalized.csv', index_col=0)\n",
        "z_test = raw_z_test.drop('patient_id',1) #Drops/deletes patient_id column\n",
        "test_labels_url = 'https://raw.githubusercontent.com/henritomas/CoE197-Z-Tomas-DL-Experiments/master/test-labels.csv'\n",
        "z_labels = pd.read_csv(test_labels_url, index_col=0)\n",
        "z_labels = to_categorical(z_labels)\n",
        "\n",
        "#Loads model at its best point and evaluates it on Final Test Data\n",
        "model = load_model('final_model_fold3.h5')\n",
        "  \n",
        "scores = model.evaluate(z_test, z_labels, batch_size=batch_size)\n",
        "print(\"val_loss: {} val_acc: {}\".format(scores[0], scores[1]))\n",
        "\n",
        "#Takes the model's probability prediction of heart disease prescence\n",
        "final_proba = model.predict(z_test)\n",
        "hd_present_proba = [prob[1] for prob in final_proba]\n",
        "print(hd_present_proba)\n",
        "\n",
        "#Stores probability associated with each patient in a csv for submission\n",
        "submission = pd.DataFrame({'heart_disease_present': hd_present_proba,\n",
        "                            'patient_id': raw_z_test.patient_id.values})\n",
        "submission = submission[['patient_id', 'heart_disease_present']]\n",
        "submission.to_csv(\"my_submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r90/90 [==============================] - 0s 3ms/step\n",
            "val_loss: 0.2834474444389343 val_acc: 0.8888888955116272\n",
            "[0.6029585, 0.06941914, 0.95167613, 0.013983928, 0.95183206, 0.018221926, 0.14276318, 0.96516067, 0.16590169, 0.060598593, 0.16051987, 0.5993903, 0.3471231, 0.97468376, 0.12118039, 0.044034027, 0.009959205, 0.0302823, 0.9356648, 0.030572087, 0.9351477, 0.19196522, 0.2217533, 0.06961808, 0.46480715, 0.95400065, 0.10646738, 0.21047239, 0.6457642, 0.013986788, 0.9526828, 0.47091955, 0.8025575, 0.5152754, 0.23172657, 0.054696035, 0.34962296, 0.057299275, 0.13667071, 0.052689783, 0.9755053, 0.022186643, 0.94820076, 0.053005967, 0.94574356, 0.041678105, 0.102881394, 0.123889275, 0.16437013, 0.83646, 0.6097066, 0.02274888, 0.9915555, 0.0630765, 0.5575688, 0.06162806, 0.9090783, 0.06646026, 0.08949696, 0.5786575, 0.07392717, 0.9763367, 0.07060773, 0.9826621, 0.07142755, 0.8949409, 0.8755535, 0.61311483, 0.9055527, 0.8438389, 0.09458623, 0.9835993, 0.97225404, 0.988846, 0.99634224, 0.9802633, 0.9610296, 0.95830256, 0.17307587, 0.51778984, 0.5680831, 0.04286138, 0.6047546, 0.87019044, 0.23100069, 0.015394269, 0.8708537, 0.7339705, 0.33145258, 0.24738567]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}